{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Signal Processing and Deep-Learning\n",
    "\n",
    "--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "- DSP/Audio Features for ML: [Valerio Velardo](https://www.youtube.com/watch?v=rlypsap6Wow&list=PL-wATfeyAMNqIee7cH3q1bh4QJFAaeNv0&index=8)\n",
    "- [DSP: Basics](https://support.ircam.fr/docs/AudioSculpt/3.0/co/Sampling.html) - Aliasing, Nyquist Freq.,lowest detectable frequency\n",
    "- A signal sampled with a 32 KHz SR, any freq. components > 16 KHz (N.F.), we get an aliasing\n",
    "- Nyquist Frequency and the relation between sampling-rate and max. frequency\n",
    "    - $F_{max} = Sampling Rate/2$\n",
    "    - $F_{max}$ is called _Nyquist Frequency_\n",
    "- Wav length (size) of np array = SR * duration of clip\n",
    "\n",
    "**Sound perception**: - [link](http://physics.bu.edu/~duffy/py105/Sound.html), [link](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2013.00636.x)\n",
    "- We perceive sound lograthimically\n",
    "- **Weber-Fechner law**: Above a minimal threshold of perception $S_0$, perceived intensity $P$ is logarithmic to stimulus intensity $S$: $P = K. log (\\frac{S}{S_0})$\n",
    "\n",
    "### Time domain features\n",
    "\n",
    "- Amplitude Envelope\n",
    "- RMS Energy\n",
    "\n",
    "\n",
    "### FFT\n",
    "\n",
    "- $s(t) = A_1.sin(2\\pi.f_1.t + \\phi_1) + A_2.sin(2\\pi.f_2.t + \\phi_2) + \\ldots{}$ \n",
    "- FFT: Y-axis: Magnitude, X-Axis: Freq.\n",
    "- Plotting a sine wave using freqs. [link](https://stackoverflow.com/questions/22566692/python-how-to-plot-graph-sine-wave)\n",
    "- When plotted - it shows a reflection around Nyquist Frequency = SR/2.\n",
    "- Reflection = aliasing\n",
    "\n",
    "### STFT\n",
    "\n",
    "- Compute FFT at several intervals \n",
    "- Interval = Frame length\n",
    "- Preserves time domain even though it is a freq. transformation!!\n",
    "- SSFT gives **Spectorgram**\n",
    "- 3 axes\n",
    "- Y: Freq., X: Time, Color variation: Magnitude\n",
    "\n",
    "\n",
    "### Mel Frequency Cepstral Coefficients (MFCC)\n",
    "\n",
    "- Timbral/textural features of sound\n",
    "- _Frequency_ domain feature\n",
    "- Approx. _human_ auditory system\n",
    "- 13-40 coeffs.\n",
    "- Calc. at each _frame_\n",
    "- Humans are able to detect piano vs violin playing same pitch/freq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "1. Convolution = dot product\n",
    "2. Center sq. replaced with value\n",
    "3. Zero padding used to resolve the edge values\n",
    "4. Kernels are **learnt**\n",
    "5. Kernels help detect features - e.g. edges\n",
    "6. KERNEL - Archtectural decision:\n",
    "    - Grid size: Kernel i.e. 3x3, 5x5. Note odd size to get center.\n",
    "    - Stride:    Kernel step size in pixels\n",
    "    - Depth:     Mono = 1, RGB = 3\n",
    "    - Number of kernels: \n",
    "7. Conv. layer output:\n",
    "    - Conv. layer has **multiple** kernels\n",
    "    - Each kernel outputs a 2D array (kernel .dot. image is also a 2D output)\n",
    "    - Output from a layer = 2D arrays equal to no.of kernels used\n",
    "8. Pooling output:\n",
    "    - Downsample the image\n",
    "    - Max./avg. pooling\n",
    "    - No parameters unlike kernel. No learning of pooling op. reqd. - we use a simple max. or avg. - nothing to learn in that\n",
    "9. POOLING - Archtectural decision:\n",
    "    - Grid size: 2x2, 3x3 etc.\n",
    "    - Stride\n",
    "    - Type i.e. max. or avg. - _Generally max. used. Helps invariance_\n",
    "\n",
    "\n",
    "10. **Architecture:**\n",
    "```\n",
    "    Input image -> {(Conv.layer+ReLU)->Pooling} -> {(Conv.layer+ReLU)->Pooling} ->  ... \n",
    "                  \\__________________ FEATURE LEARNING _____________________________________/\n",
    "                  \n",
    "     ... Flatten output -> Fully connected layers ->  Softmax -> Classification output \n",
    "        \\__________________ CLASSIFICATION  _________________/ \n",
    "```    \n",
    "\n",
    "11. First layers = low level features (lines, circles). As we progress, abstract features (wheel, headlights)\n",
    "\n",
    "12. Data shape for MFCCs:\n",
    "    Example:\n",
    "    - 13 features\n",
    "    - hop length = 512\n",
    "    - num. of samples in audio file = 51200\n",
    "    \n",
    "    Data shape = 100 x 13 x 1\n",
    "    How many time-windows = full .wav length / hop-length = 51200/512 = 100\n",
    "    Features extracted per time-window = 13\n",
    "    Depth = 1\n",
    "    \n",
    "    Therefore: Data shape = \n",
    "        {(Full track length in no. of samples) / hop-length} \n",
    "            x {No. of MFCC features}\n",
    "            x {depth of image. Mono=1, RGB=3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
